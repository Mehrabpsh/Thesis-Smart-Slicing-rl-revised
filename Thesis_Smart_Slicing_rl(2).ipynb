{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUfpyCqF94z_"
   },
   "source": [
    "# Thesis Code : Smart Slicing through Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jf792QHF9s4q"
   },
   "source": [
    "## Insall the Packages and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IruLlEn6-V3q",
    "outputId": "bd4b164b-84d3-495f-e871-bd85a0625530"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehrab/virne/lib/python3.12/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'experiment': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data': {'p_net_setting': {'dataset_dir': '', 'topology': {'type': 'erdos_renyi', 'num_nodes': 5, 'p': 1}, 'node_attrs_setting': [{'name': 'cpu', 'distribution': 'uniform', 'dtype': 'float', 'low': 8, 'high': 16}], 'link_attrs_setting': [{'name': 'bandwidth', 'distribution': 'uniform', 'dtype': 'float', 'low': 768, 'high': 1280}, {'name': 'delay', 'distribution': 'uniform', 'dtype': 'float', 'low': 10, 'high': 20}], 'output': {'if_save': True, 'save_dir': 'datasets/p_net'}}, 'v_sim_setting': {'num_v_nets': 4, 'num_groups': 4, 'cache_size': '${data.v_sim_setting.num_v_nets}', 'cache_path': '.vnReqs_cache', 'sfc_len': {'low': 5, 'high': 5, 'distribution': 'uniform'}, 'vnf_types': ['fw', 'nat', 'ids', 'wanopt'], 'node_attrs_setting': [{'name': 'cpu', 'distribution': 'uniform', 'dtype': 'float', 'low': 0, 'high': 0}], 'qos_attrs_setting': [{'name': 'bandwidth', 'distribution': 'uniform', 'dtype': 'float', 'low': 16, 'high': 256}, {'name': 'latency', 'distribution': 'uniform', 'dtype': 'float', 'low': 50, 'high': 90}], 'arrival_rate': {'distribution': 'poisson', 'lam': 1.0}, 'lifetime': {'distribution': 'exponential', 'scale': 100.0}, 'output': {'if_save': True, 'save_dir': 'datasets/v_nets', 'events_file_name': 'events.json', 'setting_file_name': 'v_sim_setting.json'}}}, 'env': {'state': {'encoder': 'NormalizedStateEncoder', 'encoder_config': {'max_latency_budget': 100}}, 'action': {'type': 'node_selection', 'mask_illegal': False}, 'reward': {'name': 'qoe_qos', 'penalty_failure': 10.0, 'penalty_exponent': True, 'penalty_exp_factor': 1.0, 'penalty_weight': 1.0, 'qoe_weight': 1.0}, 'qoe_model': {'name': 'qoe_qos_paper', 'qoe_weight_delay': 0.5, 'qoe_weight_bandwidth': 0.5, 'alpha_n': -0.006931471805599453, 'beta_n': 0.0, 'gamma_n': 4.0, 'theta_n': 1.0, 'alpha_p': 0.02484, 'beta_p': 1.185, 'gamma_p': 1.923, 'theta_p': 1.0}}, 'model': {'type': 'dqn', 'network': {'type': 'mlp', 'hidden_sizes': [256, 256], 'activation': 'relu'}, 'optimizer': {'name': 'adam', 'lr': 0.0003}, 'dqn': {'gamma': 0.99, 'buffer_size': 1000, 'batch_size': 32, 'eps_start': 1.0, 'eps_end': 0.05, 'eps_decay_steps': 10000, 'target_update': 10, 'double': False, 'dueling': False, 'n_step': 1}}, 'train': {'train': False, 'episodes': '${data.v_sim_setting.num_groups}', 'max_steps_per_episode': 200000, 'log_interval': 5, 'eval_every': 1, 'save_every': 50, 'run_in_notebook': True}, 'eval': {'enabled': True, 'policies': [{'name': 'dqn', 'type': 'dqn', 'checkpoint': None}, {'name': 'random', 'type': 'random'}, {'name': 'exhaustive', 'type': 'exhaustive', 'timeout_seconds': 5.0, 'max_embeddings': 1000}], 'metrics': ['acceptance_ratio', 'response_time', 'qoe'], 'runs': '${data.v_sim_setting.num_groups}', 'seed': 42, 'report': {'csv': True, 'tensorboard': False, 'plots': True}, 'output_dir': 'output_eval', 'dqn': {'eps_start': '${model.dqn.eps_end}', 'eps_end': '${model.dqn.eps_end}', 'eps_decay_steps': 1, 'double': '${model.dqn.double}', 'dueling': '${model.dqn.dueling}', 'n_step': 1}}, 'seed': 42, 'output_dir': 'outputs', 'project_name': 'Train-CrossValidation0', 'End_phase': 'All'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import hydra\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from hydra import compose, initialize_config_dir\n",
    "#from sfc_rl.cli import main\n",
    "\n",
    "overrides_list = [\n",
    "    \"data.v_sim_setting.num_groups=4\",\n",
    "    \"data.v_sim_setting.num_v_nets=4\",\n",
    "    \"model.dqn.eps_decay_steps=10000\",\n",
    "    \"train.run_in_notebook=true\",\n",
    "    \"eval.enabled=true\",\n",
    "    \"train.train=false\",\n",
    "    \"model.dqn.batch_size=32\",\n",
    "    \"model.dqn.buffer_size=1000\",\n",
    "    \"train.save_every=50\",\n",
    "    \"eval.policies.2.name=exhaustive\",\n",
    "    \"project_name=Train-CrossValidation0\"\n",
    "]\n",
    "\n",
    "with initialize_config_dir(version_base=None, config_dir=\"/home/mehrab/Workspaces/Thesis-Smart-Slicing-rl-revised/config\"):\n",
    "    # Pass the list of overrides to the 'overrides' argument\n",
    "    cfg = compose(config_name=\"experiment\", overrides=overrides_list)\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QMb5R23VJOpK"
   },
   "outputs": [],
   "source": [
    "#!python main(cfg) project_name=First_colab_run -> i cant run it since i pass cfg directly to main\n",
    "#from sfc_rl.cli import main\n",
    "#main(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "h95doE8WGlA1"
   },
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "from sfc_rl.data.dataset_provider import DatasetProvider\n",
    "from sfc_rl.data.dataset_generator import Generator\n",
    "from sfc_rl.env.sfc_env import SFCEnvRevised\n",
    "from sfc_rl.env.state_encoders import NormalizedStateEncoder\n",
    "from sfc_rl.env.action_space import NodeSelectionActionSpace\n",
    "from sfc_rl.env.reward import QoE_QoS_Reward\n",
    "from sfc_rl.env.qoe import QoE_QoS_PaperModel\n",
    "from sfc_rl.models.dqn import DQNPolicy\n",
    "from sfc_rl.models.networks import MLPPolicyNetwork\n",
    "from sfc_rl.models.replay_buffer import ReplayBuffer\n",
    "from sfc_rl.baselines.random_policy import RandomPolicy\n",
    "from sfc_rl.baselines.exhaustive_solver import ExhaustiveSolver\n",
    "#from sfc_rl.train.trainer import TrainerRevised\n",
    "from sfc_rl.train.evaluator import Evaluator\n",
    "from sfc_rl.utils.seed import set_seed\n",
    "from sfc_rl.utils.logging import setup_logger\n",
    "from sfc_rl.utils.tensorboard import launch_tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ndt_tnFUVLfE"
   },
   "source": [
    "## Config Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "olg3xgxuVKar",
    "outputId": "2ba66451-b6cb-487c-f7b5-8a0a0f88e0d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m 17:58:35 [sfc_rl_test_data] Starting experiment with config:\n",
      "data:\n",
      "  p_net_setting:\n",
      "    dataset_dir: ''\n",
      "    topology:\n",
      "      type: erdos_renyi\n",
      "      num_nodes: 5\n",
      "      p: 1\n",
      "    node_attrs_setting:\n",
      "    - name: cpu\n",
      "      distribution: uniform\n",
      "      dtype: float\n",
      "      low: 8\n",
      "      high: 16\n",
      "    link_attrs_setting:\n",
      "    - name: bandwidth\n",
      "      distribution: uniform\n",
      "      dtype: float\n",
      "      low: 768\n",
      "      high: 1280\n",
      "    - name: delay\n",
      "      distribution: uniform\n",
      "      dtype: float\n",
      "      low: 10\n",
      "      high: 20\n",
      "    output:\n",
      "      if_save: true\n",
      "      save_dir: datasets/p_net\n",
      "  v_sim_setting:\n",
      "    num_v_nets: 4\n",
      "    num_groups: 4\n",
      "    cache_size: ${data.v_sim_setting.num_v_nets}\n",
      "    cache_path: .vnReqs_cache\n",
      "    sfc_len:\n",
      "      low: 5\n",
      "      high: 5\n",
      "      distribution: uniform\n",
      "    vnf_types:\n",
      "    - fw\n",
      "    - nat\n",
      "    - ids\n",
      "    - wanopt\n",
      "    node_attrs_setting:\n",
      "    - name: cpu\n",
      "      distribution: uniform\n",
      "      dtype: float\n",
      "      low: 0\n",
      "      high: 0\n",
      "    qos_attrs_setting:\n",
      "    - name: bandwidth\n",
      "      distribution: uniform\n",
      "      dtype: float\n",
      "      low: 16\n",
      "      high: 256\n",
      "    - name: latency\n",
      "      distribution: uniform\n",
      "      dtype: float\n",
      "      low: 50\n",
      "      high: 90\n",
      "    arrival_rate:\n",
      "      distribution: poisson\n",
      "      lam: 1.0\n",
      "    lifetime:\n",
      "      distribution: exponential\n",
      "      scale: 100.0\n",
      "    output:\n",
      "      if_save: true\n",
      "      save_dir: datasets/v_nets\n",
      "      events_file_name: events.json\n",
      "      setting_file_name: v_sim_setting.json\n",
      "env:\n",
      "  state:\n",
      "    encoder: NormalizedStateEncoder\n",
      "    encoder_config:\n",
      "      max_latency_budget: 100\n",
      "  action:\n",
      "    type: node_selection\n",
      "    mask_illegal: false\n",
      "  reward:\n",
      "    name: qoe_qos\n",
      "    penalty_failure: 10.0\n",
      "    penalty_exponent: true\n",
      "    penalty_exp_factor: 1.0\n",
      "    penalty_weight: 1.0\n",
      "    qoe_weight: 1.0\n",
      "  qoe_model:\n",
      "    name: qoe_qos_paper\n",
      "    qoe_weight_delay: 0.5\n",
      "    qoe_weight_bandwidth: 0.5\n",
      "    alpha_n: -0.006931471805599453\n",
      "    beta_n: 0.0\n",
      "    gamma_n: 4.0\n",
      "    theta_n: 1.0\n",
      "    alpha_p: 0.02484\n",
      "    beta_p: 1.185\n",
      "    gamma_p: 1.923\n",
      "    theta_p: 1.0\n",
      "model:\n",
      "  type: dqn\n",
      "  network:\n",
      "    type: mlp\n",
      "    hidden_sizes:\n",
      "    - 256\n",
      "    - 256\n",
      "    activation: relu\n",
      "  optimizer:\n",
      "    name: adam\n",
      "    lr: 0.0003\n",
      "  dqn:\n",
      "    gamma: 0.99\n",
      "    buffer_size: 1000\n",
      "    batch_size: 32\n",
      "    eps_start: 1.0\n",
      "    eps_end: 0.05\n",
      "    eps_decay_steps: 10000\n",
      "    target_update: 10\n",
      "    double: false\n",
      "    dueling: false\n",
      "    n_step: 1\n",
      "train:\n",
      "  train: false\n",
      "  episodes: ${data.v_sim_setting.num_groups}\n",
      "  max_steps_per_episode: 200000\n",
      "  log_interval: 5\n",
      "  eval_every: 1\n",
      "  save_every: 50\n",
      "  run_in_notebook: true\n",
      "eval:\n",
      "  enabled: true\n",
      "  policies:\n",
      "  - name: dqn\n",
      "    type: dqn\n",
      "    checkpoint: null\n",
      "  - name: random\n",
      "    type: random\n",
      "  - name: exhaustive\n",
      "    type: exhaustive\n",
      "    timeout_seconds: 5.0\n",
      "    max_embeddings: 1000\n",
      "  metrics:\n",
      "  - acceptance_ratio\n",
      "  - response_time\n",
      "  - qoe\n",
      "  runs: ${data.v_sim_setting.num_groups}\n",
      "  seed: 42\n",
      "  report:\n",
      "    csv: true\n",
      "    tensorboard: false\n",
      "    plots: true\n",
      "  output_dir: output_eval\n",
      "  dqn:\n",
      "    eps_start: ${model.dqn.eps_end}\n",
      "    eps_end: ${model.dqn.eps_end}\n",
      "    eps_decay_steps: 1\n",
      "    double: ${model.dqn.double}\n",
      "    dueling: ${model.dqn.dueling}\n",
      "    n_step: 1\n",
      "seed: 42\n",
      "output_dir: outputs\n",
      "project_name: Train-CrossValidation0\n",
      "End_phase: All\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Set seed\n",
    "    seed = cfg.get(\"seed\", 42)\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = Path(f\"{cfg.get('output_dir', 'outputs')}/{cfg.get('project_name', 'Ciriaa')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Setup logger\n",
    "    logger = setup_logger(\"sfc_rl_test_data\", log_file=output_dir / \"run.log\")\n",
    "    logger.info(f\"Starting experiment with config:\\n{OmegaConf.to_yaml(cfg)}\")\n",
    "\n",
    "    # Save config\n",
    "    OmegaConf.save(cfg, output_dir / \"config.yaml\")\n",
    "\n",
    "    # Load dataset\n",
    "    # Merge data config into main config for Generator\n",
    "    # When using Hydra defaults, cfg.data contains the merged config from data/synthetic_small.yaml\n",
    "    with open_dict(cfg):\n",
    "        if 'p_net_setting' not in cfg:\n",
    "            # Check if p_net_setting is in cfg.data (from Hydra defaults)\n",
    "            if hasattr(cfg, 'data') and cfg.data is not None:\n",
    "                if hasattr(cfg.data, 'p_net_setting') and cfg.data.p_net_setting is not None:\n",
    "                    cfg.p_net_setting = cfg.data.p_net_setting\n",
    "                elif hasattr(cfg.data, 'get') and cfg.data.get('p_net_setting') is not None:\n",
    "                    cfg.p_net_setting = cfg.data.get('p_net_setting')\n",
    "\n",
    "        if 'v_sim_setting' not in cfg:\n",
    "            # Check if v_sim_setting is in cfg.data (from Hydra defaults)\n",
    "            if hasattr(cfg, 'data') and cfg.data is not None:\n",
    "                if hasattr(cfg.data, 'v_sim_setting') and cfg.data.v_sim_setting is not None:\n",
    "                    cfg.v_sim_setting = cfg.data.v_sim_setting\n",
    "                elif hasattr(cfg.data, 'get') and cfg.data.get('v_sim_setting') is not None:\n",
    "                    cfg.v_sim_setting = cfg.data.get('v_sim_setting')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pSQJsU4VhAX"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lbp9Dh-aVZBd",
    "outputId": "36c83683-5d91-4f09-8f82-f107affbc8eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m 17:58:35 [sfc_rl_test_data] PN with 5 nodes and 10 links Loaded from path \u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m 17:58:36 [sfc_rl_test_data]  16 VN requests Generated\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Create dataset provider\n",
    "    dataset_provider = DatasetProvider(cfg, logger)\n",
    "    cache_path = Path(cfg.data.v_sim_setting.get('cache_path', '.vnReqs_cache'))\n",
    "\n",
    "    if cache_path.exists() and cache_path.is_dir():\n",
    "        shutil.rmtree(cache_path)\n",
    "        logger.info(f\"Cleaned and removed cache: {cache_path}\")\n",
    "\n",
    "    pn = dataset_provider.get_physical_network()\n",
    "    vn_requests = dataset_provider.get_vn_requests()\n",
    "    flag_pn = cfg.data.p_net_setting.get('dataset_dir',None) is not None\n",
    "    flag_vn = cfg.data.v_sim_setting.get('dataset_dir',None) is not None\n",
    "    logger.info(f\"PN with {len(pn.nodes)} nodes and {len(pn.links)} links {'Loaded from path ' if flag_pn else 'Generated' }\")\n",
    "    logger.info(f\" {len(vn_requests)} VN requests {'Loaded from path ' if flag_vn else 'Generated' }\")\n",
    "\n",
    "    if cfg.get('End_phase','All') == 'data' :\n",
    "        if cache_path.exists() and cache_path.is_dir():\n",
    "            shutil.rmtree(cache_path)\n",
    "            logger.info(f\"Cleaned and removed cache: {cache_path}\")\n",
    "        else:\n",
    "            logger.info(f\"Cache directory not found: {cache_path}\")\n",
    "\n",
    "        logger.info(f\"Finishied---Dont proceed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAl8xbqBVd4T"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tnrT4l6rVdOp"
   },
   "outputs": [],
   "source": [
    "\n",
    "    #------------------------------- Env------------------------------------\n",
    "\n",
    "    num_groups = cfg.data.v_sim_setting.get('num_groups')\n",
    "\n",
    "    # Create environment components\n",
    "    env_cfg = cfg.env\n",
    "\n",
    "    # State encoder\n",
    "    state_encoder_type = env_cfg.state.get(\"encoder\", \"NormalizedStateEncoder\")\n",
    "    if state_encoder_type == \"NormalizedStateEncoder\":\n",
    "        state_encoder = NormalizedStateEncoder(env_cfg.state.get(\"encoder_config\", {}))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown encoder type: {state_encoder_type}\")\n",
    "    state_dim = state_encoder.get_state_dim(pn)\n",
    "\n",
    "    # Action space\n",
    "    action_space = NodeSelectionActionSpace(\n",
    "        mask_illegal=env_cfg.action.get(\"mask_illegal\", True)\n",
    "    )\n",
    "    action_dim = action_space.get_action_dim(pn)\n",
    "\n",
    "    # QoE model\n",
    "    qoe_cfg = env_cfg.qoe_model\n",
    "    if qoe_cfg.name == \"qoe_qos_paper\":\n",
    "        qoe_model = QoE_QoS_PaperModel(qoe_cfg.get(\"config\", {}))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown QoE model: {qoe_cfg.name}\")\n",
    "\n",
    "    # Reward function\n",
    "    #reward_fn = TerminalQoEReward(qoe_model, env_cfg.reward)\n",
    "    reward_fn = QoE_QoS_Reward(qoe_model, env_cfg.reward)\n",
    "\n",
    "    # Create environment\n",
    "    env = SFCEnvRevised(\n",
    "        pn=pn,\n",
    "        vn_requests=vn_requests,\n",
    "        num_groups=num_groups,\n",
    "        state_encoder=state_encoder,\n",
    "        action_space=action_space,\n",
    "        reward_fn=reward_fn,\n",
    "        qoe_model=qoe_model,\n",
    "        max_steps_per_request=cfg.train.get(\"max_steps_per_episode\", 2000),\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imgMGyd5Vpzo"
   },
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bAkqwX0VpIT",
    "outputId": "f1895ea2-946b-433a-b5ec-b99676ffc36c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m 17:58:50 [sfc_rl_test_data] Created DQN policy on cuda\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    #------------------------------------- Policy --------------------------------\n",
    "\n",
    "\n",
    "    # Create policy\n",
    "    model_cfg = cfg.model\n",
    "    if model_cfg.type == \"dqn\":\n",
    "\n",
    "        # Create network\n",
    "        network_cfg = model_cfg.network\n",
    "        network = MLPPolicyNetwork(\n",
    "            state_dim=state_dim,\n",
    "            action_dim=action_dim,\n",
    "            hidden_sizes=network_cfg.hidden_sizes,\n",
    "            activation=network_cfg.activation,\n",
    "            dueling=model_cfg.dqn.get(\"dueling\", False),\n",
    "        )\n",
    "\n",
    "        # Create DQN policy\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        policy = DQNPolicy(\n",
    "            state_dim=state_dim,\n",
    "            action_dim=action_dim,\n",
    "            network=network,\n",
    "            config=model_cfg,\n",
    "            device=device,\n",
    "        )\n",
    "        logger.info(f\"Created DQN policy on {device}\")\n",
    "    elif model_cfg.type == \"random\":\n",
    "        policy = RandomPolicy(cfg.get(\"seed\", 42))\n",
    "    elif model_cfg.type == 'Violent':\n",
    "        policy = ExhaustiveSolver( max_embeddings=1000000, seed = cfg.get(\"seed\", 42))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_cfg.type}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Soqy86qdVzUF"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqGs3rYvVzGx",
    "outputId": "5c6b5f1d-2ff3-4286-bdef-6a07264da804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m 17:58:53 [sfc_rl_test_data] training Skipped...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------- Train -----------------------------------------\n",
    "\n",
    "from scipy import interpolate\n",
    "from sfc_rl.train.plots import plot_training_curves\n",
    "\n",
    "tensorboard_use = False\n",
    "\n",
    "if tensorboard_use:\n",
    "\n",
    "    tb_writer = SummaryWriter(log_dir=str(output_dir / \"tensorboard\"))\n",
    "    tb_process = launch_tensorboard(output_dir / \"tensorboard\")\n",
    "    logger.info(\"launching Tensorboard...\")\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "\n",
    "num_trains = 4\n",
    "grps_permutation_seeds = np.random.randint(1,50,num_trains).tolist()\n",
    "grps_permutation_seeds[0]= None\n",
    "\n",
    "RESULTS = []\n",
    "LOSSES = []\n",
    "\n",
    "# Train\n",
    "train_cfg = cfg.train\n",
    "if train_cfg.get('train', True): #or getattr(train_cfg, 'train', True):\n",
    "\n",
    "    for tr in range(num_trains):\n",
    "        env.grps_permutation_seed = grps_permutation_seeds[tr]\n",
    "        policy.refresh()\n",
    "\n",
    "        trainer = TrainerRevised(\n",
    "            env=env,\n",
    "            policy=policy,\n",
    "            config=train_cfg,\n",
    "            output_dir=output_dir,\n",
    "            summarywriter=tb_writer if tensorboard_use else None,\n",
    "            logger=logger,\n",
    "            tensorboard_use=tensorboard_use,\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Starting training loop {tr + 1}...\")\n",
    "        results, losses = trainer.train()\n",
    "\n",
    "        for i in results.keys():\n",
    "            if 'mean_episode_length' in results[i]:\n",
    "                del results[i]['mean_episode_length']\n",
    "\n",
    "        print(f'{'#\\n' * 10}')\n",
    "        logger.info(f\"Training completed. Final metrics:\")\n",
    "        for episode, metric in results.items():\n",
    "            #print(f'Episode {episode}: {metric} \\n' )\n",
    "            logger.info(f'Episode {episode}: {metric} \\n')\n",
    "        qoes=[results[i]['qoe'] for i in results.keys()]\n",
    "        RESULTS.append(qoes)\n",
    "        LOSSES.append(losses)\n",
    "\n",
    "\n",
    "    averaged_qoes = np.mean(RESULTS, axis=0)\n",
    "    loss_interpolated = []\n",
    "\n",
    "    num_episodes = len(RESULTS[0])\n",
    "    resolution_loss =  np.max([len(loss) for loss in LOSSES])\n",
    "    print('loss_interpolated: ',resolution_loss)\n",
    "\n",
    "    x_arrays = [np.linspace(0,num_episodes,len(loss)) for loss in LOSSES]\n",
    "    x_common_loss = np.linspace(0, num_episodes , resolution_loss)  # Higher resolution than any input\n",
    "\n",
    "    # Interpolate each dataset to common grid\n",
    "    y_interpolated = []\n",
    "    for x_orig, loss_orig in zip(x_arrays, LOSSES):\n",
    "        # Create interpolation function\n",
    "        interp_func = interpolate.interp1d(\n",
    "            x_orig, loss_orig,\n",
    "            kind='linear',  # or 'cubic', 'quadratic'\n",
    "            bounds_error=False,\n",
    "            fill_value='extrapolate'  # or specific value\n",
    "        )\n",
    "        # Evaluate on common grid\n",
    "        loss_interpolated.append(interp_func(x_common_loss))\n",
    "\n",
    "    # Average the interpolated values\n",
    "    loss_avg = np.mean(loss_interpolated, axis=0)\n",
    "    loss_std = np.std(loss_interpolated, axis=0)  # Optional: standard deviation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plot_training_curves(rewards=averaged_qoes, losses= loss_avg,output_dir = output_dir / \"training plots\", filename=f\"averaged_QoE_Loss_for_{num_trains}-time-training.png\", window=5,episode=num_episodes,run_in_notebook=train_cfg.run_in_notebook)\n",
    "\n",
    "\n",
    "else:\n",
    "    logger.info(\"training Skipped...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdQomBSnV-et"
   },
   "source": [
    "## Loading best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xbIhDvVWGsz"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zytSxyCjWKLD",
    "outputId": "c4f4d3e0-484c-4b49-e6c9-da226fd835d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    \u001b[0m 17:58:54 [sfc_rl_test_data] Starting Evaluating...\u001b[0m\n",
      "\u001b[32mINFO    \u001b[0m 17:58:54 [sfc_rl_test_data] Evaluating policy: random\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating random policy :   0%|                          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group id 0 --> group id 0\n",
      "vn_requests[0:4]\n",
      "request id: 0 of group 0 failed with reward -10.0\n",
      "request id: 1 of group 0 failed with reward -10.0\n",
      "request id: 2 of group 0 embedded with reward 3.516257606759355\n",
      "request id: 3 of group 0 failed with reward -10.0\n",
      "group id 1 --> group id 1\n",
      "vn_requests[4:8]\n",
      "request id: 0 of group 1 failed with reward -10.0\n",
      "request id: 1 of group 1 failed with reward -10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating random policy :  50%|█████████         | 2/4 [00:00<00:00, 14.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request id: 2 of group 1 failed with reward -10.0\n",
      "request id: 3 of group 1 failed with reward -10.0\n",
      "group id 2 --> group id 2\n",
      "vn_requests[8:12]\n",
      "request id: 0 of group 2 embedded with reward 3.634637662689047\n",
      "request id: 1 of group 2 failed with reward -10.0\n",
      "request id: 2 of group 2 failed with reward -10.0\n",
      "request id: 3 of group 2 embedded with reward 2.165500637997977\n",
      "group id 3 --> group id 3\n",
      "vn_requests[12:16]\n",
      "request id: 0 of group 3 failed with reward -10.0\n",
      "request id: 1 of group 3 failed with reward -10.0\n",
      "request id: 2 of group 3 failed with reward -10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating random policy : 100%|██████████████████| 4/4 [00:00<00:00, 10.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request id: 3 of group 3 embedded with reward 3.1858659065513066\n",
      "\u001b[32mINFO    \u001b[0m 17:58:54 [sfc_rl_test_data] Evaluating policy: violent\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating violent policy :   0%|                         | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group id 0 --> group id 0\n",
      "vn_requests[0:4]\n",
      "request id: 0 of group 0 embedded with reward 3.0966119705031048\n",
      "request id: 1 of group 0 embedded with reward 3.7814442654148346\n",
      "request id: 2 of group 0 embedded with reward 0.34484260490702034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating violent policy :  25%|████▎            | 1/4 [00:08<00:25,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request id: 3 of group 0 failed with reward -10.0\n",
      "group id 1 --> group id 1\n",
      "vn_requests[4:8]\n",
      "request id: 0 of group 1 embedded with reward 3.8507658378028022\n",
      "request id: 1 of group 1 failed with reward -10.0\n",
      "request id: 2 of group 1 embedded with reward 4.069355144972941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating violent policy :  50%|████████▌        | 2/4 [00:17<00:17,  8.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request id: 3 of group 1 embedded with reward 2.9635984284267467\n",
      "group id 2 --> group id 2\n",
      "vn_requests[8:12]\n",
      "request id: 0 of group 2 embedded with reward 3.047783956988376\n",
      "request id: 1 of group 2 embedded with reward 4.088728613560552\n",
      "request id: 2 of group 2 failed with reward -10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating violent policy :  75%|████████████▊    | 3/4 [00:28<00:09,  9.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request id: 3 of group 2 failed with reward -10.0\n",
      "group id 3 --> group id 3\n",
      "vn_requests[12:16]\n",
      "request id: 0 of group 3 embedded with reward -3.9500317005282763\n",
      "request id: 1 of group 3 embedded with reward 3.9904462610228943\n"
     ]
    }
   ],
   "source": [
    "from sfc_rl.train.evaluator import Evaluator\n",
    "\n",
    "eval_cfg = cfg.eval\n",
    "\n",
    "\n",
    "logger.info(\"Starting Evaluating...\")\n",
    "evaluator = Evaluator(env,\n",
    "policies={'random':RandomPolicy(model_cfg.get(\"seed\", 42)),'violent':ExhaustiveSolver( max_embeddings=1000000, seed = model_cfg.get(\"seed\", 42))},\n",
    "config=eval_cfg, output_dir= output_dir / Path(eval_cfg.get('output_dir','output_eval')),logger = logger)\n",
    "\n",
    "evaluator.evaluate()\n",
    "\n",
    "logger.info(f\"Experiment completed. Results saved to {output_dir / Path(eval_cfg.get('output_dir','output_eval')) }\")\n",
    "if cache_path.exists() and cache_path.is_dir():\n",
    "    shutil.rmtree(cache_path)\n",
    "    logger.info(f\"Cleaned and removed cache: {cache_path}\")\n",
    "else:\n",
    "    logger.info(f\"Cache directory not found: {cache_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
